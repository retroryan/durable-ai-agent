# ==============================================================================
# TEMPORAL CONFIGURATION
# ==============================================================================
# Temporal server address (use 'temporal:7233' for Docker, 'localhost:7233' for local)
TEMPORAL_ADDRESS=localhost:7233
TEMPORAL_NAMESPACE=default

# Optional Temporal TLS configuration
TEMPORAL_TLS_CERT=
TEMPORAL_TLS_KEY=
TEMPORAL_API_KEY=

# ==============================================================================
# API CONFIGURATION
# ==============================================================================
API_PORT=8000
API_HOST=0.0.0.0
API_URL=http://localhost:8000

# ==============================================================================
# FRONTEND CONFIGURATION
# ==============================================================================
FRONTEND_PORT=3000
VITE_API_TIMEOUT=120000

# ==============================================================================
# WORKER CONFIGURATION
# ==============================================================================
WORKER_TASK_QUEUE=durable-ai-agent-tasks

# ==============================================================================
# TOOL CONFIGURATION
# ==============================================================================
# Available tool sets: agriculture, ecommerce, events
TOOL_SET=agriculture

# Enable mock mode for tools (true/false)
TOOLS_MOCK=true

# ==============================================================================
# DEBUG CONFIGURATION
# ==============================================================================
# Enable demo debug output
DEMO_DEBUG=true

# Enable DSPy debug mode (shows prompts/responses)
DSPY_DEBUG=false

# ==============================================================================
# LLM PROVIDER CONFIGURATION
# ==============================================================================
# Choose your LLM provider: claude, openai, ollama, gemini
LLM_PROVIDER=ollama

# Common LLM settings
LLM_TEMPERATURE=0.2
LLM_MAX_TOKENS=15000

# Generic model configuration (if using custom provider)
# LLM_MODEL=custom-provider/model-name

# ==============================================================================
# PROVIDER-SPECIFIC CONFIGURATION
# ==============================================================================

# --- Anthropic Claude Configuration ---
# ANTHROPIC_API_KEY=your-api-key-here
# CLAUDE_MODEL=claude-3-opus-20240229
# Available models:
# - claude-3-opus-20240229
# - claude-3-sonnet-20240229
# - claude-3-haiku-20240307
# - claude-2.1
# - claude-2.0
# - claude-instant-1.2

# --- OpenAI Configuration ---
# OPENAI_API_KEY=your-api-key-here
# OPENAI_MODEL=gpt-4-turbo-preview
# Available models:
# - gpt-4-turbo-preview
# - gpt-4-1106-preview
# - gpt-4
# - gpt-3.5-turbo
# - gpt-3.5-turbo-16k

# --- Google Gemini Configuration ---
# GOOGLE_API_KEY=your-api-key-here
# GEMINI_MODEL=gemini-1.5-pro-latest
# Available models:
# - gemini-1.5-pro-latest
# - gemini-1.5-flash-latest
# - gemini-1.0-pro

# --- Ollama Configuration (Local Models) ---
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=gemma3:27b
# Available Ollama models (pull with 'ollama pull <model>'):
# - gemma3:27b          # High-quality reasoning (default)
# - llama3.2:latest     # Latest Llama 3.2
# - llama3.1:8b         # Llama 3.1 8B
# - deepseek-r1:14b     # DeepSeek R1 14B
# - deepseek-r1:8b      # DeepSeek R1 8B (smaller)
# - qwen3:8b            # Qwen 3 8B
# - mistral:7b          # Mistral 7B
# - llama3.2:1b         # Llama 3.2 1B (lightweight)
# - gemma3:latest       # Latest Gemma 3
# - llama3.3:latest     # Latest Llama 3.3
# - phi3:latest         # Microsoft Phi-3
# - codellama:latest    # Code-specialized model
# - dolphin-mistral:latest  # Uncensored Mistral variant

# ==============================================================================
# MCP (MODEL CONTEXT PROTOCOL) CONFIGURATION
# ==============================================================================
# Single MCP server URL for all weather/agriculture tools
# Use 'http://mcp-server:7778/mcp' for Docker
# Use 'http://localhost:7778/mcp' for local development
MCP_SERVER_URL=http://localhost:7778/mcp

# Enable mock weather mode (true/false)
MOCK_WEATHER=true

# ==============================================================================
# LOGGING CONFIGURATION
# ==============================================================================
LOG_LEVEL=INFO

# ==============================================================================
# STRIPE CONFIGURATION (Optional)
# ==============================================================================
# Used by MCP Stripe integration if enabled
# STRIPE_API_KEY=your-stripe-api-key-here

# ==============================================================================
# LOCAL DEVELOPMENT NOTES
# ==============================================================================
# When running the agentic_loop demo with local MCP server:
#
# 1. Start MCP server locally:
#    poetry run python scripts/run_mcp_servers.py
#
# 2. Run the demo:
#    poetry run python agentic_loop/demo_react_agent.py agriculture